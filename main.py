import os
import sys
import whisper
import faiss
import numpy as np
import multiprocessing
import time
from tqdm import tqdm
from moviepy.video.io.VideoFileClip import VideoFileClip
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer

TRANSCRIPT_FILE = "transcript.txt"
SUMMARY_FILE = "summary.md"

# --- STEP 1: Extract audio from video ---
def extract_audio(video_path, audio_path="audio.wav"):
    clip = VideoFileClip(video_path)
    clip.audio.write_audiofile(audio_path)
    return audio_path

# --- STEP 2: Transcribe using Whisper ---
def transcribe_audio(audio_path):
    model = whisper.load_model("base")
    result = model.transcribe(audio_path, language="ru")
    with open(TRANSCRIPT_FILE, "w", encoding="utf-8") as f:
        f.write(result["text"])
    return result["text"]

# --- STEP 3: Split transcript into chunks ---
def split_text(text, max_words=500):
    import re
    sentences = re.split(r'(?<=[.!?]) +', text)
    chunks, chunk = [], ""
    for sentence in sentences:
        if len(chunk.split()) + len(sentence.split()) <= max_words:
            chunk += " " + sentence
        else:
            chunks.append(chunk.strip())
            chunk = sentence
    if chunk:
        chunks.append(chunk.strip())
    return chunks

# --- STEP 4: Build vector index ---
embed_model = SentenceTransformer('all-MiniLM-L6-v2')

def build_index(chunks):
    embeddings = embed_model.encode(chunks)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(np.array(embeddings))
    return index, embeddings, chunks

# --- STEP 5: Search for similar chunks ---
def search_similar_chunks(query, index, chunks, embeddings, k=3):
    query_emb = embed_model.encode([query])
    D, I = index.search(query_emb, k)
    return [chunks[i] for i in I[0]]

# --- STEP 6: Load LLM ---
llm_model = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
tokenizer = AutoTokenizer.from_pretrained(llm_model)
model = AutoModelForCausalLM.from_pretrained(llm_model, trust_remote_code=True)
llm = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=500)

# --- STEP 7: Ask question ---
def answer_question(question, relevant_chunks):
    context = "\n".join(relevant_chunks)
    prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–í–æ–ø—Ä–æ—Å: {question}\n–û—Ç–≤–µ—Ç:"
    response = llm(prompt)[0]['generated_text']
    return response.split("–û—Ç–≤–µ—Ç:")[-1].strip()

# --- STEP 8: Summarize meeting ---
# --- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º ---
def split_text_with_overlap(text, max_words=1500, overlap_words=300):
    import re
    sentences = re.split(r'(?<=[.!?]) +', text)

    chunks = []
    chunk = []
    chunk_len = 0

    for sentence in sentences:
        words = sentence.split()
        if chunk_len + len(words) <= max_words:
            chunk.append(sentence)
            chunk_len += len(words)
        else:
            chunks.append(' '.join(chunk))
            # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
            overlap = []
            overlap_len = 0
            for sent in reversed(chunk):
                sent_words = sent.split()
                overlap_len += len(sent_words)
                overlap.insert(0, sent)
                if overlap_len >= overlap_words:
                    break
            chunk = overlap + [sentence]
            chunk_len = sum(len(s.split()) for s in chunk)

    if chunk:
        chunks.append(' '.join(chunk))
    return chunks

def summarize_chunk(chunk):
    prompt = f"–í–æ—Ç —á–∞—Å—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –≤—Å—Ç—Ä–µ—á–∏:\n{chunk}\n\n–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —ç—Ç–æ–π —á–∞—Å—Ç–∏ –Ω–∞ 5-7 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –ò—Å–ø–æ–ª—å–∑—É–π —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –∏ –Ω–µ –∑–∞–¥–∞–≤–∞–π –≤–æ–ø—Ä–æ—Å—ã"
    response = llm(prompt)[0]['generated_text']
    return response

def summarize_transcript_sequential(transcript, max_words=1500, overlap_words=300):
    chunks = split_text_with_overlap(transcript, max_words=max_words, overlap_words=overlap_words)
    print(f"üîπ –†–∞–∑–±–∏—Ç–æ –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è...")

    partial_summaries = []
    for i, chunk in enumerate(chunks):
        print(f"üî∏ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç—å {i+1}/{len(chunks)}...")
        summary = summarize_chunk(chunk)
        partial_summaries.append(summary)

    full_prompt = "–í–æ—Ç –∫—Ä–∞—Ç–∫–∏–µ —Ä–µ–∑—é–º–µ —á–∞—Å—Ç–µ–π –≤—Å—Ç—Ä–µ—á–∏:\n\n" + "\n\n".join(partial_summaries) + \
                  "\n\n–ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö —Ä–µ–∑—é–º–µ —Å–¥–µ–ª–∞–π –ø–æ–ª–Ω–æ–µ –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –≤—Å–µ–π –≤—Å—Ç—Ä–µ—á–∏, –≤—ã–¥–µ–ª–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –∏ –ø—Ä–∏–Ω—è—Ç—ã–µ —Ä–µ—à–µ–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É–π —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –∏ –Ω–µ –∑–∞–¥–∞–≤–∞–π –≤–æ–ø—Ä–æ—Å—ã"
    final_summary = llm(full_prompt)[0]['generated_text']
    return final_summary
    
# --- MAIN ---
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python automeeting.py –ø—É—Ç—å_–∫_–≤–∏–¥–µ–æ.mp4\n")
        sys.exit(1)

    video_path = sys.argv[1]
    print("[1/6] –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ...")
    audio_path = extract_audio(video_path)

    print("[2/6] –†–∞—Å—à–∏—Ñ—Ä–æ–≤—ã–≤–∞–µ–º –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ Whisper...")
    transcript = transcribe_audio(audio_path)

    print("[3/6] –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –∏ —Å—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å...")
    chunks = split_text(transcript)
    index, embeddings, chunk_store = build_index(chunks)

    print("[4/6] –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –≤—Å—Ç—Ä–µ—á–∏...")
    summary = summarize_transcript_map_reduce_multiprocessing(transcript)
    with open(SUMMARY_FILE, "w", encoding="utf-8") as f:
        f.write("# üìù –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –≤—Å—Ç—Ä–µ—á–∏\n\n" + summary)

    print("\n‚úÖ –†–µ–∑—é–º–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ summary.md")
    print("üóíÔ∏è –ü–æ–ª–Ω–∞—è —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ transcript.txt")

    while True:
        print("\n–•–æ—Ç–∏—Ç–µ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ –≤—Å—Ç—Ä–µ—á–µ? (y/n)")
        if input().lower() != 'y':
            break
        question = input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å: ")
        relevant = search_similar_chunks(question, index, chunk_store, embeddings)
        answer = answer_question(question, relevant)
        print("\nü§ñ –û—Ç–≤–µ—Ç:\n", answer)

    print("\n–ì–æ—Ç–æ–≤–æ ‚ú®")
