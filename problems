Я понимаю ваше неудобство. Каждый раз вручную запускать модели Ollama внутри Docker-контейнера — это неэффективно. Проблема в том, что Docker-контейнер Verba не знает, какие модели вы хотите использовать в Ollama, и сам Ollama не держит модели "запущенными" постоянно после перезагрузки; он загружает их по требованию.

Чтобы Docker-контейнер Verba "видел" нужные модели Ollama при старте без ручного вмешательства, есть несколько подходов, но ни один из них не является "запуском" моделей внутри контейнера Verba, так как Ollama работает как отдельный сервис. Вместо этого, мы будем имитировать "запуск" или предварительную загрузку моделей.

Вот несколько способов, как это можно автоматизировать:

Вариант 1: Предварительная загрузка моделей Ollama при запуске контейнера Verba (Рекомендуемый)
Этот метод использует команду ollama pull для загрузки моделей при старте контейнера Verba, используя depends_on в docker-compose.yml, чтобы убедиться, что сервис Ollama уже запущен.

Убедитесь, что Ollama настроен как отдельный сервис в docker-compose.yml и его порт доступен для Verba.
Из ваших docker-compose.yml видно, что ollama-docker сеть используется, и Verba настроена на OLLAMA_URL=http://host.docker.internal:11434. Это хорошо, но host.docker.internal работает только на некоторых системах Docker (Docker Desktop). Если вы используете Linux, вам, возможно, придется заменить его на IP-адрес хоста или на имя сервиса ollama в compose, если вы запустили Ollama как сервис в том же docker-compose.yml.

Предположим, у вас есть сервис Ollama в том же docker-compose.yml (это лучший подход):

YAML

# docker-compose.yml
services:
  verba:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - 8000:8000
    environment:
      # ... другие переменные окружения
      - OLLAMA_URL=http://ollama:11434 # Измените на имя сервиса Ollama, если он в том же compose
      - OLLAMA_MODEL=$OLLAMA_MODEL # Модель генерации
      - OLLAMA_EMBED_MODEL=$OLLAMA_EMBED_MODEL # Модель для эмбеддингов
    depends_on:
      weaviate:
        condition: service_healthy
      ollama: # <--- Добавьте зависимость от Ollama
        condition: service_healthy
    # ... остальная часть настройки Verba
    networks:
      - ollama-docker

  ollama: # <--- Сервис Ollama, если его нет
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama # Для сохранения моделей
    command: tail -f /dev/null # Держит контейнер запущенным
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - ollama-docker

  weaviate:
    # ... настройки Weaviate

volumes:
  weaviate_data:
  ollama_data: # <--- Добавьте том для Ollama
networks:
  ollama-docker:
Измените Dockerfile Verba для предварительной загрузки моделей:
В Dockerfile Verba, после установки зависимостей, вы можете добавить команду, которая будет выполнять ollama pull для указанных моделей.

Dockerfile

# Dockerfile (для сервиса Verba)
# ... (существующий код Dockerfile)

# Устанавливаем зависимости Python
RUN pip install --no-cache-dir -r requirements.txt

# Добавляем скрипт для ожидания и загрузки моделей Ollama
COPY scripts/wait_for_ollama_and_pull_models.sh /usr/local/bin/wait_for_ollama_and_pull_models.sh
RUN chmod +x /usr/local/bin/wait_for_ollama_and_pull_models.sh

# ... (копирование остального кода)

# Измените CMD на скрипт, который сначала загружает модели, а затем запускает Verba
CMD ["/usr/local/bin/wait_for_ollama_and_pull_models.sh"]
Создайте скрипт scripts/wait_for_ollama_and_pull_models.sh:
Этот скрипт будет ждать, пока сервис Ollama станет доступным, а затем выполнит команды ollama pull для моделей, указанных в переменных окружения.

Создайте папку scripts в корне вашего проекта Verba (там же, где Dockerfile и docker-compose.yml), и в ней создайте файл wait_for_ollama_and_pull_models.sh со следующим содержимым:

Bash

#!/bin/bash
set -e

# URL сервиса Ollama, как указано в docker-compose.yml
OLLAMA_URL_HOST=${OLLAMA_URL:-http://ollama:11434} # Используем значение по умолчанию, если не задано

echo "Waiting for Ollama at $OLLAMA_URL_HOST to be ready..."
# Простая проверка доступности Ollama
until curl --output /dev/null --silent --head --fail "$OLLAMA_URL_HOST"; do
  echo "Ollama is unavailable - sleeping"
  sleep 5
done
echo "Ollama is ready!"

# Предварительная загрузка моделей Ollama, если они указаны
if [ -n "$OLLAMA_MODEL" ]; then
  echo "Pulling OLLAMA_MODEL: $OLLAMA_MODEL"
  ollama -v pull "$OLLAMA_MODEL" || true # `|| true` чтобы не падать, если модель уже есть
fi

if [ -n "$OLLAMA_EMBED_MODEL" ]; then
  echo "Pulling OLLAMA_EMBED_MODEL: $OLLAMA_EMBED_MODEL"
  ollama -v pull "$OLLAMA_EMBED_MODEL" || true
fi

echo "Starting Verba application..."
# Запускаем основное приложение Verba
exec verba
Важно: Убедитесь, что переменные OLLAMA_MODEL и OLLAMA_EMBED_MODEL заданы в вашем .env файле или в docker-compose.yml для сервиса verba.

Пример .env файла:

OLLAMA_MODEL=llama2
OLLAMA_EMBED_MODEL=nomic-embed-text
Как это работает:

При старте docker-compose up, контейнер ollama запустится первым (если он есть в compose).

Контейнер verba будет ждать, пока ollama станет "здоровым" (service_healthy в depends_on).

Как только ollama готов, скрипт wait_for_ollama_and_pull_models.sh внутри контейнера verba начнет выполнение.

Этот скрипт самостоятельно ждет доступности Ollama (через curl).

Затем он выполняет ollama pull для каждой указанной модели. Поскольку ollama/ollama образ уже содержит клиент ollama, эта команда будет работать. Модели будут загружены в том ollama_data.

После загрузки моделей скрипт запускает ваше приложение verba.

Таким образом, при каждом запуске docker compose up -d --build (или без --build, если образы уже созданы), Docker автоматически скачает нужные модели в Ollama, если их там еще нет, прежде чем Verba начнет с ними работать.

Вариант 2: Запуск Ollama с предварительно загруженными моделями (менее гибкий)
Этот подход заключается в создании собственного Docker-образа Ollama, который уже содержит нужные модели.

Создайте свой Dockerfile.ollama:

Dockerfile

# Dockerfile.ollama
FROM ollama/ollama:latest

# Модели, которые вы хотите включить
ENV OLLAMA_MODELS="llama2 nomic-embed-text"

RUN for model_name in $OLLAMA_MODELS; do \
    /usr/bin/ollama pull "$model_name" || echo "Warning: Failed to pull $model_name. It might already exist or there was a network issue." ; \
done

CMD ["/usr/bin/ollama", "serve"]
Обновите docker-compose.yml для использования вашего кастомного образа Ollama:

YAML

# docker-compose.yml
services:
  # ...
  ollama:
    build:
      context: .
      dockerfile: Dockerfile.ollama # <--- Используйте ваш кастомный Dockerfile
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama # Для сохранения моделей, чтобы не перекачивать каждый раз
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - ollama-docker
  # ...
volumes:
  ollama_data:
  # ...
Плюсы и минусы Варианта 2:

Плюсы: Модели гарантированно присутствуют внутри образа Ollama. Не требуется дополнительный скрипт в Verba.

Минусы: Создание образа может занимать много времени и места. Если вы решите использовать другие модели, вам придется пересобирать образ ollama. Менее гибко для динамического изменения моделей.

Рекомендация:

Вариант 1 (предварительная загрузка при запуске Verba с ollama pull) является более предпочтительным из-за его гибкости и простоты управления моделями через переменные окружения, не требуя пересборки образа Ollama при каждой смене модели.

Выберите тот вариант, который лучше подходит для вашего рабочего процесса, и обновите соответствующие файлы. После этого выполните docker compose up -d --build (чтобы убедиться, что все изменения Dockerfile и скриптов будут применены).
