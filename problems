Отличная новость, что вам удалось решить предыдущую проблему!

Теперь, что касается внедрения модели fluently-lm/FluentlyLM-Prinum в Verba через Ollama. Поскольку этой модели нет в официальной библиотеке Ollama, вам потребуется загрузить файл модели GGUF с Hugging Face и создать для нее Modelfile.

Вот пошаговая инструкция:

Шаг 1: Скачайте файл GGUF модели
Перейдите на страницу модели на Hugging Face: https://huggingface.co/fluently-lm/FluentlyLM-Prinum

Перейдите во вкладку "Files and versions".

В списке файлов найдите один с расширением .gguf (например, fluently-lm-prinum-q4_k_m.gguf). Выберите подходящую для вас квантованную версию (например, Q4_K_M.gguf для баланса между размером и производительностью).

Нажмите на стрелку загрузки рядом с выбранным файлом, чтобы скачать его.

Шаг 2: Подготовьте директорию для моделей Ollama
Рядом с вашим файлом docker-compose.yml создайте новую директорию, например, ollama_models.

Bash

mkdir ollama_models
Переместите скачанный файл .gguf в эту новую директорию ollama_models.

Шаг 3: Обновите docker-compose.yml для монтирования директории моделей
Чтобы Ollama могла получить доступ к вашим моделям, нужно смонтировать новую директорию ollama_models в контейнер Ollama.

Откройте ваш файл docker-compose.yml.

Найдите секцию сервиса ollama.

В секции volumes добавьте или измените строку так, чтобы она монтировала вашу новую директорию ollama_models в /root/.ollama/models внутри контейнера:

YAML

  ollama:
    image: ollama/ollama:latest
    ports:
      - 11434:11434
    volumes:
      - ollama_data:/root/.ollama
      - ./ollama_models:/root/.ollama/models # <--- ДОБАВЬТЕ ЭТУ СТРОКУ
    container_name: ollama
    pull_policy: always
    tty: true
    restart: always
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - ollama-docker
Шаг 4: Создайте Modelfile для FluentlyLM-Prinum
Modelfile сообщает Ollama, как загружать и взаимодействовать с вашей моделью.

Внутри созданной вами директории ollama_models (той, куда вы поместили файл .gguf) создайте новый текстовый файл с именем, например, Modelfile-prinum.

Откройте Modelfile-prinum и добавьте следующее содержимое. Важно: замените fluently-lm-prinum-q4_k_m.gguf на точное имя файла .gguf, который вы скачали.

Dockerfile

FROM ./fluently-lm-prinum-q4_k_m.gguf # Убедитесь, что это имя вашего .gguf файла
PARAMETER stop "<|eot_id|>"
PARAMETER stop "<|start_header_id|>"
PARAMETER stop "<|end_header_id|>"
TEMPLATE """{{ if .System }}<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{{ .System }}<|eot_id|>{% endif %}<|begin_of_text|><|start_header_id|>user<|end_header_id|>

{{ .Prompt }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
FROM ./ваш_файл.gguf: Указывает путь к файлу модели относительно Modelfile.

PARAMETER stop: Эти параметры указывают Ollama, какие токены использовать для остановки генерации, что важно для корректного форматирования диалога.

TEMPLATE: Это шаблон промпта, который Ollama будет использовать для взаимодействия с моделью. Он основан на "chat template" модели и адаптирован для Ollama.

Шаг 5: Перезапустите сервис Ollama и создайте модель в нем
Чтобы изменения в docker-compose.yml вступили в силу, а Ollama могла использовать Modelfile:

Полностью перезапустите только сервис Ollama:

Bash

docker compose up -d --build ollama
Это перезапустит Ollama и применит новое монтирование тома.

Создайте модель в Ollama, используя ваш Modelfile:

Bash

docker exec ollama ollama create fluently-lm-prinum -f /root/.ollama/models/Modelfile-prinum
fluently-lm-prinum - это имя, под которым модель будет зарегистрирована в Ollama. Вы можете выбрать любое имя.

f /root/.ollama/models/Modelfile-prinum указывает Ollama, где находится ваш Modelfile внутри контейнера.

Убедитесь, что модель появилась в списке Ollama:

Bash

docker exec ollama ollama list
Вы должны увидеть fluently-lm-prinum в списке.

Шаг 6: Обновите .env файл (или docker-compose.yml)
Теперь, когда модель доступна в Ollama, сообщите Verba, какую модель использовать.

Откройте ваш файл .env.

Измените значение OLLAMA_MODEL на имя, которое вы использовали при создании модели в Шаге 5 (например, fluently-lm-prinum):

OLLAMA_MODEL=fluently-lm-prinum
OLLAMA_EMBED_MODEL=nomic-embed-text # Это ваша текущая модель для встраиваний. Не меняйте, если она вас устраивает.
Важно: OLLAMA_EMBED_MODEL обычно является отдельной моделью для создания встраиваний (embeddings), а OLLAMA_MODEL - для генерации текста. Если nomic-embed-text вас устраивает как модель для встраиваний, оставьте ее как есть.

Шаг 7: Перезапустите сервис Verba
Чтобы Verba загрузила новую конфигурацию:

Bash

docker compose restart verba
После этих шагов Verba должна начать использовать fluently-lm-prinum в качестве модели для генерации текста. Если возникнут проблемы, предоставьте, пожалуйста, полный вывод команд и новые логи verba.
