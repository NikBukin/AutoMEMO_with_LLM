Понял. Итак, вместо того, чтобы МЕМО суммировало весь документ или текст, мы хотим, чтобы оно использовало вводимый пользователем "шаблон" (или инструкцию, промпт) для составления МЕМО. То есть, пользователь вводит некий промпт, и этот промпт передается в генератор вместе с контекстом документа для получения ответа, соответствующего промпту.

Это очень похоже на то, как работает обычный чат с RAG, но с акцентом на форматирование или содержание ответа, задаваемого шаблоном.

Давайте скорректируем план адаптации "МЕМО" с учетом этой новой логики.

Адаптация "МЕМО" для составления ответа по шаблону (промпту)
Основное изменение здесь в том, что "шаблон" пользователя станет частью промпта, передаваемого в LLM. Документ будет служить контекстом, как и в RAG-чате.

1. Изменения во Фронтенде (Next.js)
Файл: src/app/components/MEMO/MEMOInterface.tsx (аналогично ChatInterface.tsx)

Логика:

Поле ввода:

Сохраняем поле ввода текста для пользователя (аналогично userInput в ChatInterface), но его назначение меняется: это будет поле для ввода шаблона/инструкции для МЕМО.

Возможно, также потребуется поле для выбора/идентификации документа(ов), по которым будет составляться МЕМО. Если МЕМО должно быть "по всему документу", то нужно будет либо получать его ID, либо его контент. Пока давайте предположим, что документ выбирается или подразумевается.

Отправка запроса: Вместо sendUserQuery (который, вероятно, использует WebSocket для стриминга), для МЕМО, которое выдает один итоговый ответ, мы будем использовать обычный HTTP POST запрос.

Новая функция (например, sendMemoRequest), которая будет отправлять memo_prompt (содержимое поля ввода пользователя) и document_context (контекст выбранного документа) на новый API-метод бэкенда.

Отображение результата: Вместо списка сообщений чата, MEMOInterface будет отображать один итоговый текст МЕМО. Это может быть просто textarea (только для чтения) или div.

Удаление/Адаптация ненужного:

Логику WebSocket-соединения для стриминга сообщений, если суммирование будет происходить через обычный HTTP POST/GET запрос, можно убрать или адаптировать, если стриминг не нужен для МЕМО. Для "готового" резюме обычно достаточно HTTP.

Элементы UI, специфичные для чата (например, бесконечная прокрутка истории сообщений), следует убрать.

Если ChatConfig (для RAG-компонентов) актуален для МЕМО, его можно оставить или адаптировать.

2. Изменения в Бэкенде (FastAPI)
Файл: goldenverba/server/api.py

Новый API-эндпоинт: Добавьте новый POST-эндпоинт, который будет принимать шаблон МЕМО и, опционально, контекст документа.

Python

# goldenverba/server/api.py

from goldenverba.server.types import MemoPayload, MemoResponse # <--- ДОБАВЬТЕ ЭТОТ ИМПОРТ

@app.post("/api/generate_memo") # <--- НОВЫЙ ЭНДПОИНТ
async def generate_memo(payload: MemoPayload):
    try:
        client = await client_manager.connect(payload.credentials)

        # Здесь вам нужно получить контекст документа(ов)
        # Например, если payload содержит document_uuid:
        document_content = ""
        if payload.document_uuid:
            # Предполагаем, что у вас есть метод для получения контента по UUID
            document_data = await manager.weaviate_manager.retrieve_document_content(client, payload.document_uuid)
            document_content = document_data.get("content", "") # Убедитесь, что это соответствует вашей структуре
        elif payload.text_content: # Если пользователь вводит текст напрямую
            document_content = payload.text_content
        else:
            return JSONResponse(
                content={"memo_text": "Error: Document context or UUID is required."},
                status_code=400
            )

        # Вызов менеджера для создания МЕМО
        memo_result = await manager.weaviate_manager.create_memo_with_template(
            client,
            payload.memo_template, # Шаблон от пользователя
            document_content,      # Контекст документа
            payload.rag_config     # Конфигурация LLM
        )
        return JSONResponse(
            content={
                "memo_text": memo_result,
            }
        )
    except Exception as e:
        msg.fail(f"Error during memo generation: {e}")
        return JSONResponse(
            content={
                "memo_text": f"Error: Could not generate memo. Details: {str(e)}",
            },
            status_code=500
        )
Файл: goldenverba/server/managers.py

Новая функция в менеджере: В VerbaManager или WeaviateManager (в managers.py) потребуется новая асинхронная функция, которая будет выполнять логику генерации МЕМО.

Python

# goldenverba/server/managers.py

# ... внутри класса WeaviateManager (или нового специализированного менеджера)

async def create_memo_with_template(
    self,
    client: WeaviateAsyncClient,
    memo_template: str,
    document_content: str,
    rag_config: RAGConfig = None
) -> str:
    generator_name = rag_config.generator if rag_config else "default_generator_name" # Получите актуальное имя генератора
    generator_instance = self.generators[generator_name] # Доступ к экземпляру генератора

    # Формируем полный промпт для LLM, объединяя шаблон и контекст
    # Здесь вы можете экспериментировать с промптом
    full_prompt = (
        f"Based on the following document, generate a memo according to this template/instruction:\n\n"
        f"Template/Instruction: {memo_template}\n\n"
        f"Document Content: {document_content}\n\n"
        f"Generated Memo:"
    )

    # Вызываем генератор. Предполагаем, что generate_stream может быть адаптирован
    # для одноразовой генерации, если это не стрим
    memo_response = ""
    # Здесь важно, чтобы generate_stream возвращал весь текст, если не нужен стриминг,
    # или использовать другой метод генератора, который возвращает полный текст.
    # Если generate_stream только стримит, придется собрать ответ.
    async for token_dict in generator_instance.generate_stream(queries=[full_prompt], context=[], conversation={}):
         memo_response += token_dict.get("content", "")

    return memo_response.strip()

# Также вам может понадобиться функция для получения контента документа по UUID, если её нет:
async def retrieve_document_content(self, client: WeaviateAsyncClient, document_uuid: str) -> dict:
    try:
        # Ваш код для извлечения полного содержимого документа из Weaviate по UUID
        # Пример:
        # document = client.query.get(YOUR_COLLECTION_NAME).with_where(
        #    {"path": ["id"], "operator": "Equal", "valueString": document_uuid}
        # ).do()
        # return document["data"]["Get"][YOUR_COLLECTION_NAME][0]
        # Или используйте новый API-клиент Weaviate:
        collection = client.collections.get("Verba") # Замените на имя вашей коллекции
        doc_obj = await collection.query.fetch_object_by_id(document_uuid, include_vector=False, return_properties=['text_content', 'title']) # Укажите нужные поля

        if doc_obj and doc_obj.properties:
            return {"content": doc_obj.properties.get("text_content", ""), "title": doc_obj.properties.get("title", "")}
        return {"content": "", "title": ""}
    except Exception as e:
        msg.fail(f"Error retrieving document content: {e}")
        raise
3. Новые Типы Данных
Файл: src/app/types.ts

Файл: goldenverba/server/types.py (нужно создать или добавить в существующий)

TypeScript

// src/app/types.ts

// ... (существующие типы)

// Тип для запроса на генерацию МЕМО
export type MemoPayload = {
  memo_template: string; // Шаблон/инструкция от пользователя
  document_uuid?: string; // ID документа для контекста (опционально)
  text_content?: string; // Или сам текст документа (опционально)
  credentials: Credentials;
  rag_config?: RAGConfig; // Если нужна конфигурация LLM
};

// Тип для ответа МЕМО
export type MemoResponse = {
  memo_text: string;
};
И соответствующие Pydantic-модели в goldenverba/server/types.py:

Python

# goldenverba/server/types.py

# ... (существующие импорты Pydantic)
from pydantic import BaseModel
from typing import Optional

# ... (существующие модели)

class MemoPayload(BaseModel):
    memo_template: str
    document_uuid: Optional[str] = None
    text_content: Optional[str] = None
    credentials: Credentials # Убедитесь, что Credentials уже импортирован или определен
    rag_config: Optional[RAGConfig] = None # Убедитесь, что RAGConfig уже импортирован или определен

class MemoResponse(BaseModel):
    memo_text: str
4. Обновление api.ts (фронтенд)
В src/app/api.ts добавьте функцию для вызова нового эндпоинта генерации МЕМО:

TypeScript

// src/app/api.ts

// ... (существующие импорты и функции)

import { MemoPayload, MemoResponse } from "./types"; // Обновите импорт

export const sendMemoRequest = async (
  payload: MemoPayload
): Promise<MemoResponse | null> => {
  try {
    const host = await detectHost();
    const response = await fetch(`${host}/api/generate_memo`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify(payload),
    });

    if (!response.ok) {
      console.error(`Error ${response.status}: ${response.statusText}`);
      const errorData = await response.json();
      console.error("Error details:", errorData);
      return { memo_text: `Error: ${errorData.memo_text || "Failed to generate memo."}` };
    }

    const data: MemoResponse = await response.json();
    return data;
  } catch (error) {
    console.error("Error sending memo request", error);
    return { memo_text: `Error: ${error.message || "Network or unexpected error."}` };
  }
};
Общие шаги после изменений:
Сохраните все измененные файлы.

Убедитесь, что в goldenverba/server/types.py есть все необходимые импорты и определения моделей, такие как Credentials и RAGConfig, если они используются в MemoPayload.

Пересоберите Docker-образы и перезапустите Docker Compose:

Bash

docker compose up -d --build
Это гарантирует, что все изменения во фронтенде (JS, TS) и бэкенде (Python) будут включены в работающие контейнеры.

Этот подход позволит вам генерировать МЕМО, используя пользовательский шаблон и контекст из ваших документов, что соответствует вашему новому требованию.
